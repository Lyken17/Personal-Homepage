[{"authors":["Han Cai","__Ligeng Zhu__","Song Han"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1539561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1539561600,"objectID":"3b16a1ca00eab16224caab70b2804c3b","permalink":"https://lzhu.me/publication/2019-proxyless-iclr/","publishdate":"2018-10-15T00:00:00Z","relpermalink":"/publication/2019-proxyless-iclr/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":"ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware","type":"publication"},{"authors":["__Ligeng Zhu__ *","Dazhou Guo *","Yuhang Liu","Song Wang"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"77ed07312f108e6045dde0a51da31d09","permalink":"https://lzhu.me/publication/2018-spatial-encoder/","publishdate":"2018-10-01T00:00:00Z","relpermalink":"/publication/2018-spatial-encoder/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":"Small Object Sensitive Segmentation of Urban Street Scene with Consistent Spatial Adjacency Between Object Classes","type":"publication"},{"authors":["Brian Funt","__Ligeng Zhu__"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1536969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536969600,"objectID":"99c10a5222c7732d9359a37f1fabf4c1","permalink":"https://lzhu.me/publication/2018-aic/","publishdate":"2018-09-15T00:00:00Z","relpermalink":"/publication/2018-aic/","section":"publication","summary":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.","tags":null,"title":"Does Colour Really Matter? Evaluation via Object Classification","type":"publication"},{"authors":["__Ligeng Zhu__","Ruizhi Deng","Michael Maire","Zhiwei Deng","Greg Mori","Ping Tan"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"dca5486fc778cb2fdd36db5534a92e41","permalink":"https://lzhu.me/publication/2018-sparsenet/","publishdate":"2018-09-10T00:00:00Z","relpermalink":"/publication/2018-sparsenet/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":"Sparsely Aggregated Convolutional Networks","type":"publication"},{"authors":["Mengyao Zhai","Jiacheng Chen","Ruizhi Deng","__Ligeng Zhu__","Lei Chen","Greg Mori"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1530921600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530921600,"objectID":"1e291ab214600fe23ac6972ff9349238","permalink":"https://lzhu.me/publication/2018-video-gan/","publishdate":"2018-07-07T00:00:00Z","relpermalink":"/publication/2018-video-gan/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":" Learning to Forecast Videos of Human Activity with Multi-granularity Models and Adaptive Rendering","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1525305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525305600,"objectID":"32fc8023ed2489cb7215cd7878aeb5c5","permalink":"https://lzhu.me/project/2018-thop/","publishdate":"2018-05-03T00:00:00Z","relpermalink":"/project/2018-thop/","section":"project","summary":"A tool to counter PyTorch model FLOPs.","tags":["Deep Learning","Toolbox"],"title":"THOP","type":"project"},{"authors":["__Ligeng Zhu__","Brian Funt"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1517443200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1517443200,"objectID":"0f4bd01a560ae67053b77840377713e9","permalink":"https://lzhu.me/publication/2018-colorize-color-images/","publishdate":"2018-02-01T00:00:00Z","relpermalink":"/publication/2018-colorize-color-images/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":"Colorizing Color Images","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"e395ba8e63dfb4c3ed48f95d3b541dcc","permalink":"https://lzhu.me/project/2017-mxbox/","publishdate":"2017-05-03T00:00:00Z","relpermalink":"/project/2017-mxbox/","section":"project","summary":"A simple and flexible toolbox for MXNet framework.","tags":["Toolbox"],"title":"MXBox","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1493769600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1493769600,"objectID":"95938c5528570a927132b420da9e4fd5","permalink":"https://lzhu.me/project/2016-seq2seq/","publishdate":"2017-05-03T00:00:00Z","relpermalink":"/project/2016-seq2seq/","section":"project","summary":"Seq2seq + attention model for English to Chinese machine translation. ","tags":["Deep Learning","NLP"],"title":"Neural Machine Translation","type":"project"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"8157f46629a8c0026a39d652b5cfac26","permalink":"https://lzhu.me/talk/2018-neural-style/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/2018-neural-style/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":null,"title":"Deep Learning Tutorial for Beginners","type":"talk"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"9b030e48ca6c00ea11d3c3d075331004","permalink":"https://lzhu.me/talk/2018-neural-style-2/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/2018-neural-style-2/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":null,"title":"Neural Style Transform Explained","type":"talk"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1483246800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1483246800,"objectID":"77d8a34136515641ed5641fb6f442ec0","permalink":"https://lzhu.me/talk/2018-ubc-sfu-vision-day/","publishdate":"2017-01-01T00:00:00-05:00","relpermalink":"/talk/2018-ubc-sfu-vision-day/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":null,"title":"Sparsely Aggregated Convolution Networks","type":"talk"},{"authors":null,"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"e1083dd86964b992b2a4b26acbdc346d","permalink":"https://lzhu.me/project/2016-colorization/","publishdate":"2016-11-01T00:00:00Z","relpermalink":"/project/2016-colorization/","section":"project","summary":"God says, let there be color, then the deep neural network brings the color.","tags":["Deep Learning","Color Vision"],"title":"Machine Learning for Image Colorization","type":"project"},{"authors":["Luwei Yang","__Ligeng Zhu__","Yichen Wei","Shuang Liang","Ping Tan"],"categories":null,"content":"More detail can easily be written here using Markdown and $\\rm \\LaTeX$ math code.\n","date":1474588800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1474588800,"objectID":"17360c05d92608b8ab81dd38a77c6083","permalink":"https://lzhu.me/publication/2016-bmvc/","publishdate":"2016-09-23T00:00:00Z","relpermalink":"/publication/2016-bmvc/","section":"publication","summary":"We present a mobile visual clothing search system whereby a smart phone user can either choose a social networking photo or take a new photo of a person wearing clothing of interest and search for similar clothing in a retail database. From the query image, the person is detected, clothing is segmented, and clothing features are extracted and quantized. The information is sent from the phone client to a server, where the feature vector of the query image is used to retrieve similar clothing products from online databases. The phone's GPS location is used to re-rank results by retail store location. State of the art work focuses primarily on the recognition of a diverse range of clothing offline and pays little attention to practical applications. Evaluated on a challenging dataset, the system is relatively fast and achieves promising results.","tags":null,"title":"Attribute Recognition from Adaptive Parts","type":"publication"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"c1d49de23b8b483b16038c19a8b32250","permalink":"https://lzhu.me/project/2016-neural-style/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/2016-neural-style/","section":"project","summary":"A new method for video stylization that keeps the temporal consistency and reasonable time costs","tags":["Deep Learning","Color Vision"],"title":"Fast and Coherent Neural Style Transform for Videos","type":"project"},{"authors":null,"categories":null,"content":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.\n","date":1451624400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451624400,"objectID":"7b68fccc3fc5729846b60bb866fb314d","permalink":"https://lzhu.me/talk/2017-deep-live/","publishdate":"2016-01-01T00:00:00-05:00","relpermalink":"/talk/2017-deep-live/","section":"talk","summary":"Embed your slides or video here using shortcodes. Further details can easily be added using Markdown and $\\rm \\LaTeX$ math code.","tags":null,"title":"Deep Learning Tutorial for Computer Visioners","type":"talk"}]